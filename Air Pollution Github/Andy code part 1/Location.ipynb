{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This Jupyter Notebook serves as a comprehensive report detailing the location cleaning task for this project. The project revolves around a dataset generated using GPT 3.5, which contains summaries of various cases, along with location information. The primary aim was to clean, analyze, and potentially improve the quality of the location data extracted from the text of the cases. This report outlines the objectives, methods, and results of the work carried out to fulfill these goals.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "The project had three main objectives:\n",
    "1. **Data Cleaning**: The first objective was to clean the location variables in the dataset to make them suitable for analysis. This included structuring the location data into various administrative levels for easier interpretation.\n",
    "2. **Quality Assessment**: The second objective was to qualitatively evaluate the quality of the location data that was initially present in the dataset. This involved identifying the share of unidentified or ambiguous location entries.\n",
    "3. **Potential Improvement (N/A)**: The third objective was to improve the location data, if the quality was deemed inadequate. This involved creating a new code strategy for better extraction of location information from the text of the cases. (I do believe the data looks good enough at the state level. But I would love to learn more about the level of the study and see if it is necessary to go back and improve the text mining process)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "The project was executed in a Python environment, utilizing libraries such as Pandas for data manipulation, Matplotlib for data visualization, and regular expressions for string pattern matching. The workflow was as follows:\n",
    "\n",
    "1. **Data Loading**: The dataset `chatgptfull.csv` was loaded into a Pandas DataFrame for analysis.\n",
    "2. **Initial Assessment**: An initial examination of the dataset was performed to understand the structure and identify the cleaning requirements.\n",
    "3. **Data Cleaning**: The location variables were cleaned using GPT 4 and organized into different administrative levels like Zone, State, Union Territory, etc.\n",
    "4. **Quality Assessment**: A qualitative analysis was performed on the cleaned location data. This involved calculating the share of unidentified entries across all administrative levels and visualizing this information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "import ast\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "csvpath = 'C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/raw/chatgptfull.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chatgptfull = pd.read_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/raw/chatgptfull.csv')\n",
    "chatgptfull.head()\n",
    "\n",
    "temp_folder_path = 'C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/temp/'\n",
    "if not os.path.exists(temp_folder_path):\n",
    "    os.makedirs(temp_folder_path)\n",
    "\n",
    "outputdf = pd.DataFrame()\n",
    "gpt_output = []\n",
    "\n",
    "zones, states, union_territories, autonomous_divisions, divisions, districts, subdistricts, cities = [], [], [], [], [], [], [], []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt: \n",
    "\"You are an Indian classifier. You will be given a text that may or may not contain location information. You job is to parse the text into various location categories in India. Your outputs are: \n",
    "\"The Zone is: \"\n",
    "\"The State is: \"\n",
    "\"The Union Territory is: \"\n",
    "\"The Autonomous Division is: \"\n",
    "\"The Division is: \"\n",
    "\"The District is: \"\n",
    "\"The Subdistricts is: \"\n",
    "\"The City is: \"\n",
    "When the text does not contain any location information or if the text is nonsense. You output the xxx is N/A. \n",
    "You will do the same even if the location provided only contain 1 or a few levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Function to make an API call\n",
    "def api_call(location, iteration):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an Indian classifier. You will be given a text that may or may not contain location information. You job is to parse the text into various location categories in India. Your outputs are: \\n\\\"The Zone is: \\\"\\n\\\"The State is: \\\"\\n\\\"The Union Territory is: \\\" \\n\\\"The Autonomous Division is: \\\"\\n\\\"The Division is: \\\"\\n\\\"The District is: \\\"\\n\\\"The Subdistricts is: \\\"\\n\\\"The City is: \\\"\\n\\n\\nWhen the text does not contain any location information or if the text is nonsense. You output the xxx is N/A. \\n\\nYou will do the same even if the location provided only contain 1 or a few levels.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": location\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=3000,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "\n",
    "        # Save the response as a JSON file\n",
    "        json_file_path = os.path.join(temp_folder_path, f\"{iteration}.json\")\n",
    "        with open(json_file_path, 'w') as f:\n",
    "            json.dump(response, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API call failed at iteration {iteration} with error: {e}\")\n",
    "\n",
    "# Input for starting iteration\n",
    "start_iteration = int(input(\"Enter the iteration number to start from: \"))\n",
    "\n",
    "# Loop through the DataFrame\n",
    "total_rows = len(pd.read_csv(csvpath))\n",
    "for index, row in tqdm(enumerate(pd.read_csv(csvpath).iterrows(), start=start_iteration), total=total_rows - start_iteration + 1, desc=\"Processing locations\"):\n",
    "    if index < start_iteration:\n",
    "        continue\n",
    "    location = row[1]['location']\n",
    "    api_call(location, index)\n",
    "    time.sleep(1)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempFolderPath = 'C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/temp/'\n",
    "# Create empty list to store the GPT outputs\n",
    "gpt_outputs = []\n",
    "\n",
    "# Loop through all the files in the temp folder\n",
    "for i in range(len(chatgptfull)):\n",
    "    json_file_path = os.path.join(temp_folder_path, f\"{i}.json\")\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(json_file_path):\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract the GPT response from the JSON\n",
    "        gpt_output = data.get('choices', [{}])[0].get('message', {}).get('content', \"N/A\")\n",
    "        gpt_outputs.append(gpt_output)\n",
    "    else:\n",
    "        gpt_outputs.append(\"N/A\")\n",
    "\n",
    "# Create a new DataFrame for the GPT outputs\n",
    "gpt_outputs_df = pd.DataFrame({'gptoutput': gpt_outputs})\n",
    "\n",
    "# Perform a left join on the index\n",
    "chatgptfull_merged = chatgptfull.join(gpt_outputs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_csv_path = 'C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/chatgptfull_merged.csv'\n",
    "chatgptfull_merged.to_csv(merged_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_data = chatgptfull_merged\n",
    "\n",
    "# Initialize empty lists for each administrative level\n",
    "zones = []\n",
    "states = []\n",
    "union_territories = []\n",
    "autonomous_divisions = []\n",
    "divisions = []\n",
    "districts = []\n",
    "subdistricts = []\n",
    "cities = []\n",
    "\n",
    "# Loop through the gptoutput column to extract and parse the administrative levels\n",
    "for output in merged_data['gptoutput']:\n",
    "    # Using regular expressions to find the relevant information\n",
    "    zone = re.search(r'\"The Zone is: (.*?)\"', output).group(1) if re.search(r'\"The Zone is: (.*?)\"', output) else \"N/A\"\n",
    "    state = re.search(r'\"The State is: (.*?)\"', output).group(1) if re.search(r'\"The State is: (.*?)\"', output) else \"N/A\"\n",
    "    union_territory = re.search(r'\"The Union Territory is: (.*?)\"', output).group(1) if re.search(r'\"The Union Territory is: (.*?)\"', output) else \"N/A\"\n",
    "    autonomous_division = re.search(r'\"The Autonomous Division is: (.*?)\"', output).group(1) if re.search(r'\"The Autonomous Division is: (.*?)\"', output) else \"N/A\"\n",
    "    division = re.search(r'\"The Division is: (.*?)\"', output).group(1) if re.search(r'\"The Division is: (.*?)\"', output) else \"N/A\"\n",
    "    district = re.search(r'\"The District is: (.*?)\"', output).group(1) if re.search(r'\"The District is: (.*?)\"', output) else \"N/A\"\n",
    "    subdistrict = re.search(r'\"The Subdistricts is: (.*?)\"', output).group(1) if re.search(r'\"The Subdistricts is: (.*?)\"', output) else \"N/A\"\n",
    "    city = re.search(r'\"The City is: (.*?)\"', output).group(1) if re.search(r'\"The City is: (.*?)\"', output) else \"N/A\"\n",
    "\n",
    "    # Append the extracted information to the respective lists\n",
    "    zones.append(zone)\n",
    "    states.append(state)\n",
    "    union_territories.append(union_territory)\n",
    "    autonomous_divisions.append(autonomous_division)\n",
    "    divisions.append(division)\n",
    "    districts.append(district)\n",
    "    subdistricts.append(subdistrict)\n",
    "    cities.append(city)\n",
    "\n",
    "# Add new columns to the DataFrame for each administrative level\n",
    "merged_data['Zone'] = zones\n",
    "merged_data['State'] = states\n",
    "merged_data['Union_Territory'] = union_territories\n",
    "merged_data['Autonomous_Division'] = autonomous_divisions\n",
    "merged_data['Division'] = divisions\n",
    "merged_data['District'] = districts\n",
    "merged_data['Subdistrict'] = subdistricts\n",
    "merged_data['City'] = cities\n",
    "\n",
    "# Save the DataFrame with the new columns to a CSV file\n",
    "new_csv_path = 'C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/final/location_cleaned.csv'\n",
    "merged_data.to_csv(new_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to some error, the merge is off by one row. THe first row is extra and removed manually.\n",
    "\n",
    "merged_data = pd.read_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/final/location_cleaned.csv')\n",
    "# List of administrative levels to check for N/A values\n",
    "admin_levels = ['Zone', 'State', 'Union_Territory', 'Autonomous_Division', 'Division', 'District', 'Subdistrict', 'City']\n",
    "\n",
    "# Empty dictionary to store the share of N/A and non-N/A values for each administrative level\n",
    "admin_level_summary = {}\n",
    "\n",
    "# Loop through each administrative level to calculate the share of N/A and non-N/A values\n",
    "for level in admin_levels:\n",
    "    total_entries = len(merged_data)\n",
    "    na_count = merged_data[level].isna().sum()\n",
    "    non_na_count = total_entries - na_count\n",
    "    na_share = (na_count / total_entries) * 100\n",
    "    non_na_share = (non_na_count / total_entries) * 100\n",
    "    \n",
    "    admin_level_summary[level] = {'N/A Share': na_share, 'Non-N/A Share': non_na_share}\n",
    "\n",
    "# Convert the summary to a DataFrame for better readability\n",
    "summary_df = pd.DataFrame(admin_level_summary)\n",
    "\n",
    "# Set up the figure and axis for the bar plot\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create bar plot\n",
    "summary_df.T.plot(kind='bar', stacked=True, ax=ax, color=['#FF9999', '#99FF99'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Administrative Levels', fontsize=14)\n",
    "plt.ylabel('Percentage Share (%)', fontsize=14)\n",
    "plt.title('Share of N/A and Non-N/A Values for Each Administrative Level', fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Value Type', fontsize=12)\n",
    "\n",
    "# Annotate the bars with the actual percentages\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}%\", (p.get_x() + p.get_width() / 2., p.get_y() + p.get_height() / 2),\n",
    "                ha='center', va='center', fontsize=12, color='black')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for rows where all administrative levels are N/A\n",
    "unidentified_mask = merged_data[admin_levels].isna().all(axis=1)\n",
    "\n",
    "# Count the number of entirely unidentified observations\n",
    "unidentified_count = unidentified_mask.sum()\n",
    "\n",
    "# Calculate the total number of observations\n",
    "total_observations = len(merged_data)\n",
    "\n",
    "# Calculate the share of unidentified and identified observations\n",
    "unidentified_share = (unidentified_count / total_observations) * 100\n",
    "identified_share = 100 - unidentified_share\n",
    "\n",
    "# Data for the pie chart\n",
    "labels = ['Unidentified', 'Identified']\n",
    "sizes = [unidentified_share, identified_share]\n",
    "colors = ['gold', 'yellowgreen']\n",
    "explode = (0.1, 0)  # Explode the first slice\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.2f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle.\n",
    "plt.title('Share of Unidentified Observations in Administrative Regions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the charts\n",
    "outer_sizes = [88.21, 11.79]\n",
    "inner_sizes = [75.22, 24.78]\n",
    "outer_explode = (0, 0.1)\n",
    "\n",
    "# Plot the bigger pie chart (outer circle)\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('equal')\n",
    "\n",
    "# Create the outer pie chart\n",
    "outer_pie, _, outer_autotexts = ax.pie(\n",
    "    outer_sizes, labels=None, autopct='%1.2f%%',\n",
    "    startangle=90, counterclock=False, radius=1.0,\n",
    "    colors=['yellowgreen', 'gold'], explode=outer_explode,\n",
    "    wedgeprops=dict(width=0.3, edgecolor='w')\n",
    ")\n",
    "\n",
    "# Create the inner pie chart\n",
    "inner_pie, _, inner_autotexts = ax.pie(\n",
    "    inner_sizes, labels=None, autopct='%1.2f%%',\n",
    "    startangle=90, counterclock=False, radius=0.7,\n",
    "    colors=['orange', 'lightcoral'], wedgeprops=dict(width=0.3, edgecolor='w')\n",
    ")\n",
    "\n",
    "# Create a combined legend with percentages\n",
    "legend_labels = [\n",
    "    'Identified (Yellowgreen) - 88.21%',\n",
    "    'Unidentified (Gold) - 11.79%',\n",
    "    'Error/Not Possible (Orange) - 75.22% of Unidentified',\n",
    "    'Other (Lightcoral) - 24.78% of Unidentified'\n",
    "]\n",
    "legend_colors = ['yellowgreen', 'gold', 'orange', 'lightcoral']\n",
    "legend_handles = [plt.Rectangle((0,0),1,1, color=color) for color in legend_colors]\n",
    "\n",
    "plt.legend(legend_handles, legend_labels, title=\"Legend\", loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.title('Nested Pie Chart: Share of Unidentified Observations and Error/Not Possible')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flagging cities that do not have districts and other superior levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>kanoon_id</th>\n",
       "      <th>summary_response</th>\n",
       "      <th>q1</th>\n",
       "      <th>q1_response</th>\n",
       "      <th>q2</th>\n",
       "      <th>q2_response</th>\n",
       "      <th>q3</th>\n",
       "      <th>q3_response</th>\n",
       "      <th>q4</th>\n",
       "      <th>...</th>\n",
       "      <th>location</th>\n",
       "      <th>gptoutput</th>\n",
       "      <th>Zone</th>\n",
       "      <th>State</th>\n",
       "      <th>Union_Territory</th>\n",
       "      <th>Autonomous_Division</th>\n",
       "      <th>Division</th>\n",
       "      <th>District</th>\n",
       "      <th>Subdistrict</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33640192</td>\n",
       "      <td>In a recent court order, a petitioner filed a ...</td>\n",
       "      <td>In the following summary of a judgement order,...</td>\n",
       "      <td>50: The court order did not have a direct pro-...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>No, the court did not ask the pollution contro...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>Follow: The court did not tweak the law in its...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>...</td>\n",
       "      <td>['District Dausa, State of Rajasthan.']</td>\n",
       "      <td>\"The Zone is: N/A\"\\n\"The State is: Rajasthan\"\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dausa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29160358</td>\n",
       "      <td>The Delhi High Court recently dismissed a writ...</td>\n",
       "      <td>In the following summary of a judgement order,...</td>\n",
       "      <td>50: The judgement order does not have a clear ...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>No. The court did not ask the pollution contro...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>Follow: The court did not tweak the law in its...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>...</td>\n",
       "      <td>['The jurisdiction of this judgement is in the...</td>\n",
       "      <td>\"The Zone is: N/A\"\\n\"The State is: Karnataka\"\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1518862</td>\n",
       "      <td>The court order in question relates to a bail ...</td>\n",
       "      <td>In the following summary of a judgement order,...</td>\n",
       "      <td>50: The court order does not have a clear pro-...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>No, the court did not ask the pollution contro...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>Follow: The court has not tweaked the law in i...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>...</td>\n",
       "      <td>['State of Kerala']</td>\n",
       "      <td>\"The Zone is: N/A\"\\n\"The State is: Kerala\"\\n\"T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>111869679</td>\n",
       "      <td>The court orders discussed in this summary per...</td>\n",
       "      <td>In the following summary of a judgement order,...</td>\n",
       "      <td>Score: 60\\nReason: The court orders discuss va...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>Yes: The court asked the Karnataka Industrial ...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>Follow: Based on the information provided, the...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>...</td>\n",
       "      <td>['The jurisdiction of this judgement is BENGAL...</td>\n",
       "      <td>\"The Zone is: N/A\"\\n\"The State is: Karnataka\"\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>58003943</td>\n",
       "      <td>The National Green Tribunal (NGT) heard an app...</td>\n",
       "      <td>In the following summary of a judgement order,...</td>\n",
       "      <td>50: The court order presents both sides of the...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>Yes, the court asked the pollution control boa...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>Tweak: The court tweaked the law in its judgem...</td>\n",
       "      <td>In the following judgement order summary, did ...</td>\n",
       "      <td>...</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>\"The Zone is: N/A\"\\n\"The State is: N/A\"\\n\"The ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  kanoon_id                                   summary_response  \\\n",
       "0           0   33640192  In a recent court order, a petitioner filed a ...   \n",
       "1           1   29160358  The Delhi High Court recently dismissed a writ...   \n",
       "2           2    1518862  The court order in question relates to a bail ...   \n",
       "3           3  111869679  The court orders discussed in this summary per...   \n",
       "4           4   58003943  The National Green Tribunal (NGT) heard an app...   \n",
       "\n",
       "                                                  q1  \\\n",
       "0  In the following summary of a judgement order,...   \n",
       "1  In the following summary of a judgement order,...   \n",
       "2  In the following summary of a judgement order,...   \n",
       "3  In the following summary of a judgement order,...   \n",
       "4  In the following summary of a judgement order,...   \n",
       "\n",
       "                                         q1_response  \\\n",
       "0  50: The court order did not have a direct pro-...   \n",
       "1  50: The judgement order does not have a clear ...   \n",
       "2  50: The court order does not have a clear pro-...   \n",
       "3  Score: 60\\nReason: The court orders discuss va...   \n",
       "4  50: The court order presents both sides of the...   \n",
       "\n",
       "                                                  q2  \\\n",
       "0  In the following judgement order summary, did ...   \n",
       "1  In the following judgement order summary, did ...   \n",
       "2  In the following judgement order summary, did ...   \n",
       "3  In the following judgement order summary, did ...   \n",
       "4  In the following judgement order summary, did ...   \n",
       "\n",
       "                                         q2_response  \\\n",
       "0  No, the court did not ask the pollution contro...   \n",
       "1  No. The court did not ask the pollution contro...   \n",
       "2  No, the court did not ask the pollution contro...   \n",
       "3  Yes: The court asked the Karnataka Industrial ...   \n",
       "4  Yes, the court asked the pollution control boa...   \n",
       "\n",
       "                                                  q3  \\\n",
       "0  In the following judgement order summary, did ...   \n",
       "1  In the following judgement order summary, did ...   \n",
       "2  In the following judgement order summary, did ...   \n",
       "3  In the following judgement order summary, did ...   \n",
       "4  In the following judgement order summary, did ...   \n",
       "\n",
       "                                         q3_response  \\\n",
       "0  Follow: The court did not tweak the law in its...   \n",
       "1  Follow: The court did not tweak the law in its...   \n",
       "2  Follow: The court has not tweaked the law in i...   \n",
       "3  Follow: Based on the information provided, the...   \n",
       "4  Tweak: The court tweaked the law in its judgem...   \n",
       "\n",
       "                                                  q4  ...  \\\n",
       "0  In the following judgement order summary, did ...  ...   \n",
       "1  In the following judgement order summary, did ...  ...   \n",
       "2  In the following judgement order summary, did ...  ...   \n",
       "3  In the following judgement order summary, did ...  ...   \n",
       "4  In the following judgement order summary, did ...  ...   \n",
       "\n",
       "                                            location  \\\n",
       "0            ['District Dausa, State of Rajasthan.']   \n",
       "1  ['The jurisdiction of this judgement is in the...   \n",
       "2                                ['State of Kerala']   \n",
       "3  ['The jurisdiction of this judgement is BENGAL...   \n",
       "4                                              ERROR   \n",
       "\n",
       "                                           gptoutput Zone      State  \\\n",
       "0  \"The Zone is: N/A\"\\n\"The State is: Rajasthan\"\\...  NaN  Rajasthan   \n",
       "1  \"The Zone is: N/A\"\\n\"The State is: Karnataka\"\\...  NaN  Karnataka   \n",
       "2  \"The Zone is: N/A\"\\n\"The State is: Kerala\"\\n\"T...  NaN     Kerala   \n",
       "3  \"The Zone is: N/A\"\\n\"The State is: Karnataka\"\\...  NaN  Karnataka   \n",
       "4  \"The Zone is: N/A\"\\n\"The State is: N/A\"\\n\"The ...  NaN        NaN   \n",
       "\n",
       "  Union_Territory Autonomous_Division Division   District Subdistrict  \\\n",
       "0             NaN                 NaN      NaN      Dausa         NaN   \n",
       "1             NaN                 NaN      NaN        NaN         NaN   \n",
       "2             NaN                 NaN      NaN        NaN         NaN   \n",
       "3             NaN                 NaN      NaN  Bangalore         NaN   \n",
       "4             NaN                 NaN      NaN        NaN         NaN   \n",
       "\n",
       "        City  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3  Bangalore  \n",
       "4        NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_location = pd.read_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/final/location_cleaned.csv')\n",
    "cleaned_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Union_Territory</th>\n",
       "      <th>Autonomous_Division</th>\n",
       "      <th>Division</th>\n",
       "      <th>District</th>\n",
       "      <th>Subdistrict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chennai</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>New Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Aligarh</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9416</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9429</th>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9436</th>\n",
       "      <td>Chennai</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>Pondicherry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9438</th>\n",
       "      <td>Cuttack</td>\n",
       "      <td>Orissa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1219 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           City          State Union_Territory Autonomous_Division Division  \\\n",
       "13        Delhi            NaN           Delhi                 NaN      NaN   \n",
       "25      Chennai     Tamil Nadu             NaN                 NaN      NaN   \n",
       "26    New Delhi            NaN           Delhi                 NaN      NaN   \n",
       "36      Aligarh  Uttar Pradesh             NaN                 NaN      NaN   \n",
       "43        Delhi            NaN           Delhi                 NaN      NaN   \n",
       "...         ...            ...             ...                 ...      ...   \n",
       "9416      Delhi            NaN           Delhi                 NaN      NaN   \n",
       "9429  Bengaluru      Karnataka             NaN                 NaN      NaN   \n",
       "9433      Delhi          Delhi           Delhi                 NaN      NaN   \n",
       "9436    Chennai     Tamil Nadu     Pondicherry                 NaN      NaN   \n",
       "9438    Cuttack         Orissa             NaN                 NaN      NaN   \n",
       "\n",
       "     District Subdistrict  \n",
       "13        NaN         NaN  \n",
       "25        NaN         NaN  \n",
       "26        NaN         NaN  \n",
       "36        NaN         NaN  \n",
       "43        NaN         NaN  \n",
       "...       ...         ...  \n",
       "9416      NaN         NaN  \n",
       "9429      NaN         NaN  \n",
       "9433      NaN         NaN  \n",
       "9436      NaN         NaN  \n",
       "9438      NaN         NaN  \n",
       "\n",
       "[1219 rows x 7 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the DataFrame\n",
    "filtered_df = cleaned_location[cleaned_location['City'].notna() & \n",
    "                 cleaned_location[['District']].isna().all(axis=1)]\n",
    "\n",
    "# Displaying the filtered DataFrame\n",
    "filtered_df[['City', 'State', 'Union_Territory', 'Autonomous_Division', 'Division', 'District', 'Subdistrict']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find district names from the cities in the filtered locations\n",
    "#### Making a dictionary for cities and districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Geolocator with a user agent\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "# Function to get location details by city name\n",
    "def get_location_by_city(city_name, state_name):\n",
    "    query = f\"{city_name}, {state_name}, India\" if state_name else f\"{city_name}, India\"\n",
    "    location = geolocator.geocode(query)\n",
    "    if location:\n",
    "        return location.raw\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# List of cities and states from your filtered DataFrame\n",
    "cities_to_lookup = zip(filtered_df['City'].tolist(), filtered_df['State'].tolist())\n",
    "\n",
    "# Dictionary to store the fetched details\n",
    "city_mapping = {}\n",
    "total = len(filtered_df)\n",
    "\n",
    "# Loop to find the details of each city\n",
    "for city, state in tqdm(cities_to_lookup, total=total, desc=\"Fetching location data\", ascii=False, ncols=100, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]'):  # tqdm for progress bar\n",
    "    location_data = get_location_by_city(city, state)\n",
    "    if location_data:\n",
    "        display_name = location_data['display_name']\n",
    "        # Store the details in a harmonized format, parsing as needed\n",
    "        city_mapping[city] = display_name.split(\", \")\n",
    "    else:\n",
    "        city_mapping[city] = [\"Could not find data\"]\n",
    "    \n",
    "    # Pause to respect rate limits\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mapping_df = pd.DataFrame(list(city_mapping.items()), columns=['City', 'Location_Info'])\n",
    "city_mapping_df.to_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/city_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_mapping_df[['City', 'Location_Info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "city_mapping_df = pd.DataFrame(list(city_mapping.items()), columns=['City', 'Location_Info'])\n",
    "\n",
    "# Merge the new DataFrame with the filtered_df\n",
    "merged_df = pd.merge(filtered_df, city_mapping_df, on='City', how='left')\n",
    "\n",
    "# Initialize empty columns for State, District, and other geographical levels\n",
    "for col in ['State', 'District', 'Subdistrict', 'Division', 'Union_Territory', 'Autonomous_Division']:\n",
    "    merged_df[col] = None\n",
    "\n",
    "# Function to extract relevant information based on common keywords\n",
    "def extract_info(row):\n",
    "    location_info = row['Location_Info']\n",
    "    for info in location_info:\n",
    "        if \"District\" in info:\n",
    "            row['District'] = info\n",
    "        elif \"State\" in info:\n",
    "            row['State'] = info\n",
    "        elif \"Subdistrict\" in info:\n",
    "            row['Subdistrict'] = info\n",
    "        elif \"Division\" in info:\n",
    "            row['Division'] = info\n",
    "        elif \"Union Territory\" in info:\n",
    "            row['Union_Territory'] = info\n",
    "        elif \"Autonomous Division\" in info:\n",
    "            row['Autonomous_Division'] = info\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "transformed_df = merged_df.apply(extract_info, axis=1)\n",
    "\n",
    "# Save the transformed DataFrame to a new CSV file\n",
    "transformed_df.to_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/transformed_city_mapping.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a dictionary for mapping states to districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_mapping = pd.read_excel('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/raw/India States Wiki.xlsx', sheet_name='Mapping')\n",
    "district_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_to_states = {}\n",
    "\n",
    "# Populate the dictionary\n",
    "for index, row in district_mapping.iterrows():\n",
    "    state = row['State/Union Territory']\n",
    "    districts = row['Districts'].split(', ')\n",
    "    for district in districts:\n",
    "        district_to_states[district] = state  # Corrected this line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a few states aren't IDed. Based on my observation, the state name comes right after the district names. So I will try to extract the state name from the text and then map it to the district name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract district and state information\n",
    "def extract_info(row):\n",
    "    location_info = row['Location_Info']\n",
    "    for i, info in enumerate(location_info):\n",
    "        if \"District\" in info:\n",
    "            # Extracting the district name from the string\n",
    "            district_name = info.replace(' District', '').strip()\n",
    "            row['District'] = district_name\n",
    "            # Using known districts to find state\n",
    "            if district_name in district_to_states:\n",
    "                row['State'] = district_to_states[district_name]\n",
    "            else:\n",
    "                # If state is not found in the dictionary, look for it in the next item in location_info\n",
    "                if i+1 < len(location_info):\n",
    "                    potential_state = location_info[i+1].strip()\n",
    "                    row['State'] = potential_state\n",
    "    return row\n",
    "\n",
    "# Initialize empty columns for State and District in your DataFrame\n",
    "merged_df['State'] = None\n",
    "merged_df['District'] = None\n",
    "\n",
    "# Apply the function to each row\n",
    "transformed_df = merged_df.apply(extract_info, axis=1)\n",
    "\n",
    "# Save the transformed DataFrame to a new CSV file\n",
    "transformed_df.to_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/transformed_city_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = pd.read_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/transformed_city_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the transformed_city_mapping_df into two subsets: identified and unidentified\n",
    "identified_df = transformed_df.dropna(subset=['District', 'State'])\n",
    "unidentified_df = transformed_df[pd.isna(transformed_df['District']) | pd.isna(transformed_df['State'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Merge the identified_df back into the original cleaned_location_df\n",
    "# Using 'Unnamed: 0' as the merge key in both DataFrames\n",
    "merged_cleaned_location_df = pd.merge(cleaned_location, identified_df[['Unnamed: 0', 'District', 'State']], \n",
    "                                      on='Unnamed: 0', how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Step 3: Replace the original District and State columns with the new identified data where applicable\n",
    "merged_cleaned_location_df['District'].fillna(merged_cleaned_location_df['District_new'], inplace=True)\n",
    "merged_cleaned_location_df['State'].fillna(merged_cleaned_location_df['State_new'], inplace=True)\n",
    "\n",
    "# Step 4: Drop the temporary new columns\n",
    "merged_cleaned_location_df.drop(columns=['District_new', 'State_new'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cleaned_location_df.head()\n",
    "merged_cleaned_location_df.to_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/final/merged_cleaned_location.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_loc_df = merged_cleaned_location_df.copy()\n",
    "# Update the District and State names for observations where City name is 'Delhi' or 'New Delhi'\n",
    "merged_loc_df.loc[merged_loc_df['City'].isin(['Delhi', 'New Delhi']), ['District', 'State']] = 'New Delhi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidentified_df = merged_loc_df[pd.isna(merged_loc_df['District']) | pd.isna(merged_loc_df['State'])]\n",
    "unidentified_df.head()\n",
    "unidentified_df.to_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/unidentified.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_df = merged_loc_df.dropna(subset=['District', 'State'])\n",
    "\n",
    "# Replace city names that are \"Tis Hazari Courts, Delhi\" with \"Delhi\" in the identified_df\n",
    "identified_df.loc[identified_df['City'] == 'Tis Hazari Courts, Delhi', 'City'] = 'Delhi'\n",
    "\n",
    "identified_df.to_csv('C:/Users/andre/OneDrive/Work/Georgetown RA/Joshi-DeJure/0_data/interim/identified.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
